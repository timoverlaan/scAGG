{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import joblib as jl\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src\")  # hacky way to get access to the util.save_perf\n",
    "from util import save_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad(\"data/COMBAT/COMBAT-CITESeq-DATA-top2000.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_COL = \"Donor ID\"\n",
    "CELLTYPE_COL = \"Annotation_minor_subset\"\n",
    "LABEL = \"SARSCoV2PCR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = adata.X\n",
    "y = adata.obs[LABEL].values\n",
    "donor_ids = adata.obs[SAMPLE_COL].values  # donor ID each cell belongs to\n",
    "new_donors = np.unique(donor_ids)  # unique donor IDs\n",
    "\n",
    "# now get the label of each donor in new_donors\n",
    "# this is the label of the first cell of each donor in new_donors\n",
    "donor_labels = np.array([\n",
    "    y[donor_ids == donor_id][0] for donor_id in new_donors\n",
    "])\n",
    "\n",
    "# X = x / x.sum(axis=1).reshape(-1, 1) * 1e4\n",
    "# X = np.log1p(X)\n",
    "# X = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1\n",
      "Fold 1/5\n",
      "With best threshold (gridsearch):\n",
      "acc  (cell) = 0.7523, acc  (donor) = 0.8214\n",
      "prec (cell) = 0.7762, prec (donor) = 0.8500\n",
      "rec  (cell) = 0.8916, rec  (donor) = 0.8947\n",
      "f1   (cell) = 0.8299, f1   (donor) = 0.8718\n",
      "auc  (cell) = 0.8044, auc  (donor) = 0.8655\n",
      "Fold 2/5\n",
      "With best threshold (gridsearch):\n",
      "acc  (cell) = 0.8208, acc  (donor) = 0.9643\n",
      "prec (cell) = 0.8360, prec (donor) = 0.9500\n",
      "rec  (cell) = 0.9359, rec  (donor) = 1.0000\n",
      "f1   (cell) = 0.8831, f1   (donor) = 0.9744\n",
      "auc  (cell) = 0.8824, auc  (donor) = 1.0000\n",
      "Fold 3/5\n",
      "With best threshold (gridsearch):\n",
      "acc  (cell) = 0.7715, acc  (donor) = 0.8929\n",
      "prec (cell) = 0.7603, prec (donor) = 0.8636\n",
      "rec  (cell) = 0.9601, rec  (donor) = 1.0000\n",
      "f1   (cell) = 0.8486, f1   (donor) = 0.9268\n",
      "auc  (cell) = 0.8585, auc  (donor) = 1.0000\n",
      "Fold 4/5\n",
      "With best threshold (gridsearch):\n",
      "acc  (cell) = 0.7793, acc  (donor) = 0.9643\n",
      "prec (cell) = 0.7687, prec (donor) = 0.9474\n",
      "rec  (cell) = 0.9428, rec  (donor) = 1.0000\n",
      "f1   (cell) = 0.8469, f1   (donor) = 0.9730\n",
      "auc  (cell) = 0.8697, auc  (donor) = 0.9833\n",
      "Fold 5/5\n",
      "With best threshold (gridsearch):\n",
      "acc  (cell) = 0.7718, acc  (donor) = 0.8214\n",
      "prec (cell) = 0.7664, prec (donor) = 0.8095\n",
      "rec  (cell) = 0.9213, rec  (donor) = 0.9444\n",
      "f1   (cell) = 0.8368, f1   (donor) = 0.8718\n",
      "auc  (cell) = 0.8433, auc  (donor) = 0.8722\n",
      "\n",
      "Per cell:\n",
      "Mean test  acc (cell): 0.7791 +/- 0.0000\n",
      "Mean test prec (cell): 0.7815 +/- 0.0000\n",
      "Mean test  rec (cell): 0.9304 +/- 0.0000\n",
      "Mean test   f1 (cell): 0.8491 +/- 0.0000\n",
      "Mean test  auc (cell): 0.8517 +/- 0.0000\n",
      "\n",
      "Per donor:\n",
      "Mean test  acc: 0.8929 +/- 0.0000\n",
      "Mean test prec: 0.8841 +/- 0.0000\n",
      "Mean test  rec: 0.9678 +/- 0.0000\n",
      "Mean test   f1: 0.9236 +/- 0.0000\n",
      "Mean test  auc: 0.9442 +/- 0.0000\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 1\n",
    "N_FOLDS = 5\n",
    "SPLIT_SEED = None\n",
    "# SPLIT_SEED = 42\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "mean_acc_cell = np.zeros(N_RUNS)\n",
    "mean_prec_cell = np.zeros(N_RUNS)\n",
    "mean_rec_cell = np.zeros(N_RUNS)\n",
    "mean_f1_cell = np.zeros(N_RUNS)\n",
    "mean_auc_cell = np.zeros(N_RUNS)\n",
    "\n",
    "mean_acc_donor = np.zeros(N_RUNS)\n",
    "mean_prec_donor = np.zeros(N_RUNS)\n",
    "mean_rec_donor = np.zeros(N_RUNS)\n",
    "mean_f1_donor = np.zeros(N_RUNS)\n",
    "mean_auc_donor = np.zeros(N_RUNS)\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for j in range(N_RUNS):\n",
    "    print(f\"Run {j+1}/{N_RUNS}\")\n",
    "\n",
    "    if SPLIT_SEED is not None:\n",
    "        kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SPLIT_SEED)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True)\n",
    "\n",
    "    test_acc_cell = []\n",
    "    test_acc_donor = []\n",
    "    test_prec_cell = []\n",
    "    test_prec_donor = []\n",
    "    test_rec_cell = []\n",
    "    test_rec_donor = []\n",
    "    test_f1_cell = []\n",
    "    test_f1_donor = []\n",
    "    test_auc_cell = []\n",
    "    test_auc_donor = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(new_donors, donor_labels)):\n",
    "        print(f\"Fold {i+1}/{N_FOLDS}\")\n",
    "\n",
    "        # Train/test split\n",
    "        train_donors = [new_donors[donor_idx] for donor_idx in train_index]\n",
    "        test_donors = [new_donors[donor_idx] for donor_idx in test_index]\n",
    "        x_train = x[np.isin(donor_ids, train_donors)]\n",
    "        y_train = y[np.isin(donor_ids, train_donors)]    \n",
    "        x_test = x[np.isin(donor_ids, test_donors)]\n",
    "        y_test = y[np.isin(donor_ids, test_donors)]\n",
    "        \n",
    "        # Model definition\n",
    "        model = Lasso(alpha=0.03, max_iter=1000)\n",
    "\n",
    "        # Model training\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Model evaluation\n",
    "        y_pred_train = model.predict(x_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "\n",
    "        test_auc_cell.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "        y_pred_train = (y_pred_train > 0.5).astype(int)\n",
    "        y_pred_test = (y_pred_test > 0.5).astype(int)\n",
    "                \n",
    "        # Metrics per cell\n",
    "        acc_train = (y_pred_train == y_train).mean()\n",
    "        acc_test = (y_pred_test == y_test).mean()\n",
    "    \n",
    "        test_acc_cell.append(accuracy_score(y_test, y_pred_test))\n",
    "        test_prec_cell.append(precision_score(y_test, y_pred_test))\n",
    "        test_rec_cell.append(recall_score(y_test, y_pred_test))\n",
    "        test_f1_cell.append(f1_score(y_test, y_pred_test))\n",
    "\n",
    "        # But now we want to aggregate predictions per donor\n",
    "        donor_pred = []\n",
    "        donor_true = []\n",
    "        test_donor_ids = np.array(donor_ids)[np.isin(donor_ids, test_donors)]\n",
    "        for donor in test_donors:\n",
    "            idx = np.isin(test_donor_ids, donor)\n",
    "            donor_pred.append(y_pred_test[idx].mean())\n",
    "            donor_true.append(y_test[idx].mean())\n",
    "        donor_pred = np.array(donor_pred)\n",
    "        donor_true = np.array(donor_true)\n",
    "\n",
    "        # Also for training data \n",
    "        donor_pred_train = []\n",
    "        donor_true_train = []\n",
    "        train_donor_ids = np.array(donor_ids)[np.isin(donor_ids, train_donors)]\n",
    "        for donor in train_donors:\n",
    "            idx = np.isin(train_donor_ids, donor)\n",
    "            donor_pred_train.append(y_pred_train[idx].mean())\n",
    "            donor_true_train.append(y_train[idx].mean())\n",
    "        donor_pred_train = np.array(donor_pred_train)\n",
    "        donor_true_train = np.array(donor_true_train)\n",
    "\n",
    "        # Pick best threshold based on training data:\n",
    "        thresholds = np.linspace(0, 1, 101)\n",
    "        best_threshold = 0\n",
    "        best_acc = 0\n",
    "        for threshold in thresholds:\n",
    "            donor_pred_train_ = (donor_pred_train > threshold).astype(int)\n",
    "            acc = accuracy_score(donor_true_train, donor_pred_train_)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_threshold = threshold\n",
    "\n",
    "        # Metrics per donor\n",
    "        test_auc_donor.append(roc_auc_score(donor_true, donor_pred))\n",
    "        donor_pred = (donor_pred > best_threshold).astype(int)\n",
    "\n",
    "        test_acc_donor.append(accuracy_score(donor_true, donor_pred))\n",
    "        test_prec_donor.append(precision_score(donor_true, donor_pred))\n",
    "        test_rec_donor.append(recall_score(donor_true, donor_pred))\n",
    "        test_f1_donor.append(f1_score(donor_true, donor_pred))\n",
    "\n",
    "        print(\"With best threshold (gridsearch):\")\n",
    "        print(f\"acc  (cell) = {test_acc_cell[-1]:.4f}, acc  (donor) = {test_acc_donor[-1]:.4f}\")\n",
    "        print(f\"prec (cell) = {test_prec_cell[-1]:.4f}, prec (donor) = {test_prec_donor[-1]:.4f}\")\n",
    "        print(f\"rec  (cell) = {test_rec_cell[-1]:.4f}, rec  (donor) = {test_rec_donor[-1]:.4f}\")\n",
    "        print(f\"f1   (cell) = {test_f1_cell[-1]:.4f}, f1   (donor) = {test_f1_donor[-1]:.4f}\")\n",
    "        print(f\"auc  (cell) = {test_auc_cell[-1]:.4f}, auc  (donor) = {test_auc_donor[-1]:.4f}\")\n",
    "\n",
    "        save_perf(\n",
    "            exp_name=\"COMBAT_top2000\",\n",
    "            model_name=\"Cell-level\",\n",
    "            fold=i,\n",
    "            accuracy=test_acc_donor[-1],\n",
    "            precision=test_prec_donor[-1],\n",
    "            recall=test_rec_donor[-1],\n",
    "            f1=test_f1_donor[-1],\n",
    "            roc_auc=test_auc_donor[-1],\n",
    "            train_donors=train_donors,\n",
    "            test_donors=test_donors,\n",
    "            train_y=donor_labels[train_index],\n",
    "            test_y=donor_labels[test_index],\n",
    "            train_y_pred=donor_pred_train.flatten(),\n",
    "            test_y_pred=donor_pred.flatten(),\n",
    "        )\n",
    "\n",
    "\n",
    "    mean_acc_cell[j] = np.mean(test_acc_cell)\n",
    "    mean_prec_cell[j] = np.mean(test_prec_cell)\n",
    "    mean_rec_cell[j] = np.mean(test_rec_cell)\n",
    "    mean_f1_cell[j] = np.mean(test_f1_cell)\n",
    "    mean_auc_cell[j] = np.mean(test_auc_cell)\n",
    "\n",
    "    mean_acc_donor[j] = np.mean(test_acc_donor)\n",
    "    mean_prec_donor[j] = np.mean(test_prec_donor)\n",
    "    mean_rec_donor[j] = np.mean(test_rec_donor)\n",
    "    mean_f1_donor[j] = np.mean(test_f1_donor)\n",
    "    mean_auc_donor[j] = np.mean(test_auc_donor)\n",
    "    \n",
    "print(\"\\nPer cell:\")\n",
    "print(f\"Mean test  acc (cell): {np.mean(mean_acc_cell):.4f} +/- {np.std(mean_acc_cell):.4f}\")\n",
    "print(f\"Mean test prec (cell): {np.mean(mean_prec_cell):.4f} +/- {np.std(mean_prec_cell):.4f}\")\n",
    "print(f\"Mean test  rec (cell): {np.mean(mean_rec_cell):.4f} +/- {np.std(mean_rec_cell):.4f}\")\n",
    "print(f\"Mean test   f1 (cell): {np.mean(mean_f1_cell):.4f} +/- {np.std(mean_f1_cell):.4f}\")\n",
    "print(f\"Mean test  auc (cell): {np.mean(mean_auc_cell):.4f} +/- {np.std(mean_auc_cell):.4f}\")\n",
    "\n",
    "print(\"\\nPer donor:\")\n",
    "print(f\"Mean test  acc: {np.mean(mean_acc_donor):.4f} +/- {np.std(mean_acc_donor):.4f}\")\n",
    "print(f\"Mean test prec: {np.mean(mean_prec_donor):.4f} +/- {np.std(mean_prec_donor):.4f}\")\n",
    "print(f\"Mean test  rec: {np.mean(mean_rec_donor):.4f} +/- {np.std(mean_rec_donor):.4f}\")\n",
    "print(f\"Mean test   f1: {np.mean(mean_f1_donor):.4f} +/- {np.std(mean_f1_donor):.4f}\")\n",
    "print(f\"Mean test  auc: {np.mean(mean_auc_donor):.4f} +/- {np.std(mean_auc_donor):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**LASSO (alpha=0.03), 1k genes**\n",
    "```\n",
    "100%|██████████| 10/10 [08:49<00:00, 52.93s/it]\n",
    "Mean test  acc (cell): 0.5503 +/- 0.0079\n",
    "Mean test prec (cell): 0.5618 +/- 0.0053\n",
    "Mean test  rec (cell): 0.7893 +/- 0.0142\n",
    "Mean test   f1 (cell): 0.6521 +/- 0.0066\n",
    "Mean test  auc (cell): 0.5726 +/- 0.0119\n",
    "\n",
    "Per donor:\n",
    "Mean test  acc: 0.5989 +/- 0.0309\n",
    "Mean test prec: 0.6176 +/- 0.0242\n",
    "Mean test  rec: 0.7662 +/- 0.0628\n",
    "Mean test   f1: 0.6677 +/- 0.0445\n",
    "Mean test  auc: 0.6325 +/- 0.0120\n",
    "```\n",
    "\n",
    "**LASSO (alpha=0.03), 2k genes**\n",
    "```\n",
    "Mean test  acc (cell): 0.5238 +/- 0.0099\n",
    "Mean test prec (cell): 0.5410 +/- 0.0062\n",
    "Mean test  rec (cell): 0.9025 +/- 0.0355\n",
    "Mean test   f1 (cell): 0.6656 +/- 0.0146\n",
    "Mean test  auc (cell): 0.5740 +/- 0.0075\n",
    "\n",
    "Per donor:\n",
    "Mean test  acc: 0.5568 +/- 0.0111\n",
    "Mean test prec: 0.5694 +/- 0.0124\n",
    "Mean test  rec: 0.8934 +/- 0.0451\n",
    "Mean test   f1: 0.6872 +/- 0.0140\n",
    "Mean test  auc: 0.5219 +/- 0.0201\n",
    "```\n",
    "\n",
    "**LASSO (alpha=0.03), 5k genes**\n",
    "```\n",
    "Mean test  acc (cell): 0.5406 +/- 0.0045\n",
    "Mean test prec (cell): 0.5496 +/- 0.0034\n",
    "Mean test  rec (cell): 0.8722 +/- 0.0164\n",
    "Mean test   f1 (cell): 0.6690 +/- 0.0053\n",
    "Mean test  auc (cell): 0.5680 +/- 0.0098\n",
    "\n",
    "Per donor:\n",
    "Mean test  acc: 0.5949 +/- 0.0119\n",
    "Mean test prec: 0.6003 +/- 0.0111\n",
    "Mean test  rec: 0.8609 +/- 0.0436\n",
    "Mean test   f1: 0.7006 +/- 0.0146\n",
    "Mean test  auc: 0.5764 +/- 0.0200\n",
    "```\n",
    "\n",
    "**LASSO (alpha=0.03), 8k genes**\n",
    "... cancelled due to time limit. Is now running again as job 11062857"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
